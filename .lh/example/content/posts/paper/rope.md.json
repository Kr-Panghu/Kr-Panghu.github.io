{
    "sourceFile": "example/content/posts/paper/rope.md",
    "activeCommit": 0,
    "commits": [
        {
            "activePatchIndex": 28,
            "patches": [
                {
                    "date": 1720056780184,
                    "content": "Index: \n===================================================================\n--- \n+++ \n"
                },
                {
                    "date": 1720056786546,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1,9 +1,9 @@\n ---\n title: \"The Proof of Long-Term Decay Property of ROPE\"\n tags: [\"paper\"]\n date: 2024\n-path: \"posts/paper\"\n+path: \"posts/rope\"\n excerpt: paperlist\n ---\n \n ## LLM\n"
                },
                {
                    "date": 1720056800716,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -2,9 +2,9 @@\n title: \"The Proof of Long-Term Decay Property of ROPE\"\n tags: [\"paper\"]\n date: 2024\n path: \"posts/rope\"\n-excerpt: paperlist\n+# excerpt: paperlist\n ---\n \n ## LLM\n \n"
                },
                {
                    "date": 1720120882819,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -0,0 +1,9 @@\n+---\n+title: \"The Proof of Long-Term Decay Property of ROPE\"\n+tags: [\"paper\"]\n+date: 2024\n+path: \"posts/rope\"\n+# excerpt: paperlist\n+---\n+\n+ROPE is the most popular positional embedding method that has been applied in many\n"
                },
                {
                    "date": 1720120891396,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -5,16 +5,5 @@\n path: \"posts/rope\"\n # excerpt: paperlist\n ---\n \n-ROPE is the most popular positional embedding method that has been applied in many\n----\n-title: \"The Proof of Long-Term Decay Property of ROPE\"\n-tags: [\"paper\"]\n-date: 2024\n-path: \"posts/rope\"\n-# excerpt: paperlist\n----\n-\n-## LLM\n-\n-### Token reward\n+ROPE is the most popular positional embedding method that has been applied in many po\n"
                },
                {
                    "date": 1720120897591,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -5,5 +5,5 @@\n path: \"posts/rope\"\n # excerpt: paperlist\n ---\n \n-ROPE is the most popular positional embedding method that has been applied in many po\n+ROPE is the most popular positional embedding method that has been applied in many popular\n"
                },
                {
                    "date": 1720120908339,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -0,0 +1,9 @@\n+---\n+title: \"The Proof of Long-Term Decay Property of ROPE\"\n+tags: [\"paper\"]\n+date: 2024\n+path: \"posts/rope\"\n+# excerpt: paperlist\n+---\n+\n+ROPE is the most common positional embedding method that has been applied in many popular Language Models\n"
                },
                {
                    "date": 1720120940518,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1,18 +1,13 @@\n ---\n-title: \"The Proof of Long-Term Decay Property of ROPE\"\n+title: \"The Proof of Long-Term Decay Property of RoPE\"\n tags: [\"paper\"]\n date: 2024\n path: \"posts/rope\"\n # excerpt: paperlist\n ---\n \n-ROPE is the most common positional embedding method that has been applied in many popular Language Models\n----\n-title: \"The Proof of Long-Term Decay Property of ROPE\"\n-tags: [\"paper\"]\n-date: 2024\n-path: \"posts/rope\"\n-# excerpt: paperlist\n----\n+RoPE is the most common positional embedding method that has been applied in many popular Language Models. RoPE has several properties, among which the long-term decay property is the most important one. In this paper, we prove that RoPE has the long-term decay property. We also provide a detailed analysis of the long-term decay property of RoPE and show that it is a key factor in the success of RoPE in various NLP tasks.\n \n-ROPE is the most popular positional embedding method that has been applied in many popular\n+```\n+\n+```\n"
                },
                {
                    "date": 1720121038485,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -5,9 +5,5 @@\n path: \"posts/rope\"\n # excerpt: paperlist\n ---\n \n-RoPE is the most common positional embedding method that has been applied in many popular Language Models. RoPE has several properties, among which the long-term decay property is the most important one. In this paper, we prove that RoPE has the long-term decay property. We also provide a detailed analysis of the long-term decay property of RoPE and show that it is a key factor in the success of RoPE in various NLP tasks.\n-\n-```\n-\n-```\n+RoPE is the most common positional embedding method that has been applied in many popular Language Models. RoPE has several properties, among which the long-term decay property is the most important one. Recently, I've been researching on how to make long-term decay property more customizable and controllable. In this paper, I will present the proof of the long-term decay property of RoPE and discuss the potential improvements of RoPE.\n"
                },
                {
                    "date": 1720121082522,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -5,5 +5,5 @@\n path: \"posts/rope\"\n # excerpt: paperlist\n ---\n \n-RoPE is the most common positional embedding method that has been applied in many popular Language Models. RoPE has several properties, among which the long-term decay property is the most important one. Recently, I've been researching on how to make long-term decay property more customizable and controllable. In this paper, I will present the proof of the long-term decay property of RoPE and discuss the potential improvements of RoPE.\n+RoPE is the most common positional embedding method that has been applied in many popular Language Models. RoPE has several properties, among which the long-term decay property is the most important one. Recently, I've been researching on how to make long-term decay property more customizable and controllable, which is highly relevant to RoPE. Therefore, I want to get myself really understand why RoPE has the long-term decay property and how it works. In this post, I will provide a proof of the long-term decay property of RoPE.\n"
                },
                {
                    "date": 1720121096597,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1,7 +1,7 @@\n ---\n title: \"The Proof of Long-Term Decay Property of RoPE\"\n-tags: [\"paper\"]\n+tags: [\"PE\"]\n date: 2024\n path: \"posts/rope\"\n # excerpt: paperlist\n ---\n"
                },
                {
                    "date": 1720121111496,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -5,5 +5,7 @@\n path: \"posts/rope\"\n # excerpt: paperlist\n ---\n \n+## Introduction\n+\n RoPE is the most common positional embedding method that has been applied in many popular Language Models. RoPE has several properties, among which the long-term decay property is the most important one. Recently, I've been researching on how to make long-term decay property more customizable and controllable, which is highly relevant to RoPE. Therefore, I want to get myself really understand why RoPE has the long-term decay property and how it works. In this post, I will provide a proof of the long-term decay property of RoPE.\n"
                },
                {
                    "date": 1720121175764,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -0,0 +1,11 @@\n+---\n+title: \"The Proof of Long-Term Decay Property of RoPE\"\n+tags: [\"PE\"]\n+date: 2024\n+path: \"posts/rope\"\n+# excerpt: paperlist\n+---\n+\n+## Introduction\n+\n+RoPE is the most common positional embedding method that has been applied in many popular Language Models. RoPE has several properties, among which the long-term decay property is the most important one. Recently, I've been researching on how to make long-term decay property more customizable and controllable, which is highly relevant to RoPE. Therefore, I want to get myself really understand why RoPE has the long-term decay property and how it works. In this post, I will provide a proof of the long-term decay property of RoPE. I will also cover some recent researches on RoPE and its applications.\n"
                },
                {
                    "date": 1720121185940,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -8,15 +8,6 @@\n \n ## Introduction\n \n RoPE is the most common positional embedding method that has been applied in many popular Language Models. RoPE has several properties, among which the long-term decay property is the most important one. Recently, I've been researching on how to make long-term decay property more customizable and controllable, which is highly relevant to RoPE. Therefore, I want to get myself really understand why RoPE has the long-term decay property and how it works. In this post, I will provide a proof of the long-term decay property of RoPE. I will also cover some recent researches on RoPE and its applications.\n----\n-title: \"The Proof of Long-Term Decay Property of RoPE\"\n-tags: [\"PE\"]\n-date: 2024\n-path: \"posts/rope\"\n-# excerpt: paperlist\n----\n \n-## Introduction\n-\n-RoPE is the most common positional embedding method that has been applied in many popular Language Models. RoPE has several properties, among which the long-term decay property is the most important one. Recently, I've been researching on how to make long-term decay property more customizable and controllable, which is highly relevant to RoPE. Therefore, I want to get myself really understand why RoPE has the long-term decay property and how it works. In this post, I will provide a proof of the long-term decay property of RoPE.\n+## Citation\n"
                },
                {
                    "date": 1720121193771,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -7,7 +7,7 @@\n ---\n \n ## Introduction\n \n-RoPE is the most common positional embedding method that has been applied in many popular Language Models. RoPE has several properties, among which the long-term decay property is the most important one. Recently, I've been researching on how to make long-term decay property more customizable and controllable, which is highly relevant to RoPE. Therefore, I want to get myself really understand why RoPE has the long-term decay property and how it works. In this post, I will provide a proof of the long-term decay property of RoPE. I will also cover some recent researches on RoPE and its applications.\n+RoPE^[1] is the most common positional embedding method that has been applied in many popular Language Models. RoPE has several properties, among which the long-term decay property is the most important one. Recently, I've been researching on how to make long-term decay property more customizable and controllable, which is highly relevant to RoPE. Therefore, I want to get myself really understand why RoPE has the long-term decay property and how it works. In this post, I will provide a proof of the long-term decay property of RoPE. I will also cover some recent researches on RoPE and its applications.\n \n ## Citation\n"
                },
                {
                    "date": 1720121522425,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -9,5 +9,7 @@\n ## Introduction\n \n RoPE^[1] is the most common positional embedding method that has been applied in many popular Language Models. RoPE has several properties, among which the long-term decay property is the most important one. Recently, I've been researching on how to make long-term decay property more customizable and controllable, which is highly relevant to RoPE. Therefore, I want to get myself really understand why RoPE has the long-term decay property and how it works. In this post, I will provide a proof of the long-term decay property of RoPE. I will also cover some recent researches on RoPE and its applications.\n \n-## Citation\n+## References\n+\n+[2] Ding, Yiran, et al. \"Longrope: Extending llm context window beyond 2 million tokens.\" arXiv preprint arXiv:2402.13753 (2024).\n"
                },
                {
                    "date": 1720121528695,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -11,5 +11,7 @@\n RoPE^[1] is the most common positional embedding method that has been applied in many popular Language Models. RoPE has several properties, among which the long-term decay property is the most important one. Recently, I've been researching on how to make long-term decay property more customizable and controllable, which is highly relevant to RoPE. Therefore, I want to get myself really understand why RoPE has the long-term decay property and how it works. In this post, I will provide a proof of the long-term decay property of RoPE. I will also cover some recent researches on RoPE and its applications.\n \n ## References\n \n+[1]\n+\n [2] Ding, Yiran, et al. \"Longrope: Extending llm context window beyond 2 million tokens.\" arXiv preprint arXiv:2402.13753 (2024).\n"
                },
                {
                    "date": 1720121656515,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -0,0 +1,17 @@\n+---\n+title: \"The Proof of Long-Term Decay Property of RoPE\"\n+tags: [\"PE\"]\n+date: 2024\n+path: \"posts/rope\"\n+# excerpt: paperlist\n+---\n+\n+## Introduction\n+\n+RoPE^[1] is the most common positional embedding method that has been applied in many popular Language Models. RoPE has several properties, among which the long-term decay property is the most important one. Recently, I've been researching on how to make long-term decay property more customizable and controllable, which is highly relevant to RoPE. Therefore, I want to get myself really understand why RoPE has the long-term decay property and how it works. In this post, I will provide a proof of the long-term decay property of RoPE. I will also cover some recent researches on RoPE and its applications.\n+\n+## References\n+\n+[1] Su, Jianlin, et al. \"Roformer: Enhanced transformer with rotary position embedding.\" Neurocomputing 568 (2024): 127063.\n+\n+[2] Ding, Yiran, et al. \"Longrope: Extending llm context window beyond 2 million tokens.\" arXiv preprint arXiv:2402.13753 (2024).\n"
                },
                {
                    "date": 1720122086980,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -0,0 +1,19 @@\n+---\n+title: \"The Proof of Long-Term Decay Property of RoPE\"\n+tags: [\"PE\"]\n+date: 2024\n+path: \"posts/rope\"\n+# excerpt: paperlist\n+---\n+\n+## Introduction\n+\n+RoPE[^1]\n+\n+is the most common positional embedding method that has been applied in many popular Language Models. RoPE has several properties, among which the long-term decay property is the most important one. Recently, I've been researching on how to make long-term decay property more customizable and controllable, which is highly relevant to RoPE. Therefore, I want to get myself really understand why RoPE has the long-term decay property and how it works. In this post, I will provide a proof of the long-term decay property of RoPE. I will also cover some recent researches on RoPE and its applications.\n+\n+## References\n+\n+[1] Su, Jianlin, et al. \"Roformer: Enhanced transformer with rotary position embedding.\" Neurocomputing 568 (2024): 127063.\n+\n+[2] Ding, Yiran, et al. \"Longrope: Extending llm context window beyond 2 million tokens.\" arXiv preprint arXiv:2402.13753 (2024).\n"
                },
                {
                    "date": 1720122123886,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -13,41 +13,7 @@\n is the most common positional embedding method that has been applied in many popular Language Models. RoPE has several properties, among which the long-term decay property is the most important one. Recently, I've been researching on how to make long-term decay property more customizable and controllable, which is highly relevant to RoPE. Therefore, I want to get myself really understand why RoPE has the long-term decay property and how it works. In this post, I will provide a proof of the long-term decay property of RoPE. I will also cover some recent researches on RoPE and its applications.\n \n ## References\n \n-[1] Su, Jianlin, et al. \"Roformer: Enhanced transformer with rotary position embedding.\" Neurocomputing 568 (2024): 127063.\n+[^1] Su, Jianlin, et al. \"Roformer: Enhanced transformer with rotary position embedding.\" Neurocomputing 568 (2024): 127063.\n \n [2] Ding, Yiran, et al. \"Longrope: Extending llm context window beyond 2 million tokens.\" arXiv preprint arXiv:2402.13753 (2024).\n----\n-title: \"The Proof of Long-Term Decay Property of RoPE\"\n-tags: [\"PE\"]\n-date: 2024\n-path: \"posts/rope\"\n-# excerpt: paperlist\n----\n-\n-## Introduction\n-\n-RoPE^[1] is the most common positional embedding method that has been applied in many popular Language Models. RoPE has several properties, among which the long-term decay property is the most important one. Recently, I've been researching on how to make long-term decay property more customizable and controllable, which is highly relevant to RoPE. Therefore, I want to get myself really understand why RoPE has the long-term decay property and how it works. In this post, I will provide a proof of the long-term decay property of RoPE. I will also cover some recent researches on RoPE and its applications.\n-\n-## References\n-\n-[1] Su, Jianlin, et al. \"Roformer: Enhanced transformer with rotary position embedding.\" Neurocomputing 568 (2024): 127063.\n-\n-[2] Ding, Yiran, et al. \"Longrope: Extending llm context window beyond 2 million tokens.\" arXiv preprint arXiv:2402.13753 (2024).\n----\n-title: \"The Proof of Long-Term Decay Property of RoPE\"\n-tags: [\"PE\"]\n-date: 2024\n-path: \"posts/rope\"\n-# excerpt: paperlist\n----\n-\n-## Introduction\n-\n-RoPE^[1] is the most common positional embedding method that has been applied in many popular Language Models. RoPE has several properties, among which the long-term decay property is the most important one. Recently, I've been researching on how to make long-term decay property more customizable and controllable, which is highly relevant to RoPE. Therefore, I want to get myself really understand why RoPE has the long-term decay property and how it works. In this post, I will provide a proof of the long-term decay property of RoPE. I will also cover some recent researches on RoPE and its applications.\n-\n-## References\n-\n-[1]\n-\n-[2] Ding, Yiran, et al. \"Longrope: Extending llm context window beyond 2 million tokens.\" arXiv preprint arXiv:2402.13753 (2024).\n"
                },
                {
                    "date": 1720153685144,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -7,13 +7,122 @@\n ---\n \n ## Introduction\n \n-RoPE[^1]\n+RoPE[^1] is the most common positional embedding method that has been applied in many popular Language Models. RoPE has several properties, among which the **long-term decay** property is really interesting. Recently, I've been researching on how to make long-term decay property more customizable and controllable. Therefore, I want to get myself really understand why RoPE has the long-term decay property and how it works. In this post, I will provide a proof of the long-term decay property of RoPE., and will also cover some recent researches relevant to RoPE and try to make extensions.\n \n-is the most common positional embedding method that has been applied in many popular Language Models. RoPE has several properties, among which the long-term decay property is the most important one. Recently, I've been researching on how to make long-term decay property more customizable and controllable, which is highly relevant to RoPE. Therefore, I want to get myself really understand why RoPE has the long-term decay property and how it works. In this post, I will provide a proof of the long-term decay property of RoPE. I will also cover some recent researches on RoPE and its applications.\n+## Review | Some Details\n \n+Before getting the proof, let's first recal the **Long-term decay of RoPE** (Sec.3.4.3) in the original paper[^1]. Contents below are completely copied from the orginal paper, <u>the equation number from paper is kept.</u>\n+\n+---\n+\n+We can group entries of vectors $\\boldsymbol{q}=\\boldsymbol{W}_q \\boldsymbol{x}_m$ and $\\boldsymbol{k}=\\boldsymbol{W}_k \\boldsymbol{x}_n$ in pairs, and the inner product of RoPE in Equation (16) can be written as a complex number multiplication.\n+\n+$$\n+\\left(\\boldsymbol{R}_{\\Theta, m}^d \\boldsymbol{W}_q \\boldsymbol{x}_m\\right)^{\\top}\\left(\\boldsymbol{R}_{\\Theta, n}^d \\boldsymbol{W}_k \\boldsymbol{x}_n\\right)=\\operatorname{Re}\\left[\\sum_{i=0}^{d / 2-1} \\boldsymbol{q}_{[2 i: 2 i+1]} k_{[2 i: 2 i+1]}^* e^{i(m-n) \\theta_i}\\right] \\tag{1 or 35}\n+$$\n+\n+where $\\boldsymbol{q}_{[2 i: 2 i+1]}$ represents the $2 i^{\\text {th }}$ to $(2 i+1)^{\\text {th }}$ entries of $\\boldsymbol{q}$. Denote $h_i=\\boldsymbol{q}_{[2 i: 2 i+1]} \\boldsymbol{k}_{[2 i: 2 i+1]}^*$ and $S_j=$ $\\sum_{i=0}^{j-1} e^{i(m-n) \\theta_i}$, and let $h_{d / 2}=0$ and $S_0=0$, we can rewrite the summation using Abel transformation\n+\n+$$\n+\\sum_{i=0}^{d / 2-1} \\boldsymbol{q}_{[2 i: 2 i+1]} \\boldsymbol{k}_{[2 i: 2 i+1]}^* e^{i(m-n) \\theta_i}=\\sum_{i=0}^{d / 2-1} h_i\\left(S_{i+1}-S_i\\right)=-\\sum_{i=0}^{d / 2-1} S_{i+1}\\left(h_{i+1}-h_i\\right) \\tag{2 or 36}\n+$$\n+\n+Thus,\n+\n+$$\n+\\begin{aligned}\n+\\left|\\sum_{i=0}^{d / 2-1} \\boldsymbol{q}_{[2 i: 2 i+1]} \\boldsymbol{k}_{[2 i: 2 i+1]}^* e^{i(m-n) \\theta_i}\\right| & =\\left|\\sum_{i=0}^{d / 2-1} S_{i+1}\\left(h_{i+1}-h_i\\right)\\right| \\\\\n+& \\leq \\sum_{i=0}^{d / 2-1}\\left|S_{i+1}\\right|\\left|\\left(h_{i+1}-h_i\\right)\\right| \\\\\n+& \\leq\\left(\\max _i\\left|h_{i+1}-h_i\\right|\\right) \\sum_{i=0}^{d / 2-1}\\left|S_{i+1}\\right|\n+\\end{aligned} \\tag{3 or 37}\n+$$\n+\n+Note that the value of $\\frac{1}{d / 2} \\sum_{i=1}^{d / 2}\\left|S_i\\right|$ decay with the relative distance $m-n$ increases by setting $\\theta_i=10000^{-2 i / d}$, as shown in Figure (2).\n+\n+---\n+\n+I believe some readers will have common questions with me, it seems in this part of paper, **many details are elliminated for simplicity.** I don't want to be the guy who just pretend these details are not important, focus on ideas, I will present more derivations here, since so far I haven't seen any detailed explanations here, including the blog of the author[^2].\n+\n+Let's revise some annotation here. $\\boldsymbol{R}_{\\theta, m}^d$ is the following matrix:\n+\n+$$\n+\\boldsymbol{R}_{\\theta, m}^d=\\left(\\begin{array}{ccccccc}\n+\\cos m \\theta_1 & -\\sin m \\theta_1 & 0 & 0 & \\cdots & 0 & 0 \\\\\n+\\sin m \\theta_1 & \\cos m \\theta_1 & 0 & 0 & \\cdots & 0 & 0 \\\\\n+0 & 0 & \\cos m \\theta_2 & -\\sin m \\theta_2 & \\cdots & 0 & 0 \\\\\n+0 & 0 & \\sin m \\theta_2 & \\cos m \\theta_2 & \\cdots & 0 & 0 \\\\\n+\\vdots & \\vdots & \\vdots & \\vdots & \\ddots & \\vdots & \\vdots \\\\\n+0 & 0 & 0 & 0 & \\cdots & \\cos m \\theta_{d / 2} & -\\sin m \\theta_{d / 2} \\\\\n+0 & 0 & 0 & 0 & \\cdots & \\sin m \\theta_{d / 2} & \\cos m \\theta_{d / 2}\n+\\end{array}\\right)_{d\\times d}\n+$$\n+\n+$\\theta_i$ belongs to $\\Theta=\\left\\{\\theta_i=10000^{-2(i-1) / d}, i \\in[1,2, \\ldots, d / 2]\\right\\}$​​, where 10000 is called **\"base\"**, as a predefined constant. The value of base will have interesting effect on RoPE, which we will discuss later.\n+\n+$\\boldsymbol{x}_i \\in \\mathbb{R}^d$ is the d-dimensional word embedding vector of token $w_i$ (Let $\\mathbb{S}_N=\\left\\{w_i\\right\\}_{i=1}^N$ be a sequence of $N$ input tokens with $w_i$ being the $i^{t h}$ element. The corresponding word embedding of $\\mathbb{S}_N$ is denoted as $\\mathbb{E}_N=\\left\\{\\boldsymbol{x}_i\\right\\}_{i=1}^N$). $\\boldsymbol{q}=\\boldsymbol{W}_q \\boldsymbol{x}_m$ and $\\boldsymbol{k}=\\boldsymbol{W}_k \\boldsymbol{x}_n$ are of the shape as $\\boldsymbol{x_i}$.\n+\n+**Now let's first derive Eq.(35).** We first try to get **its complete form without using complex numbers**. Each block $\\boldsymbol{R_{m\\theta_i}}$ in matrix $\\boldsymbol{R_{\\theta, m}^d}$ is a rotation matrix, where $\\boldsymbol{R_{m\\theta_i}}=\\left(\\begin{array}{cc}\n+\\cos m \\theta_1 & -\\sin m \\theta_1 \\\\\n+\\sin m \\theta_1 & \\cos m \\theta_1\n+\\end{array}\\right)$ means contrarotating $m\\theta_1$, while\n+\n+$$\n+{\\boldsymbol{R_{n\\theta_i}}}^T=\\left(\\begin{array}{cc}\n+\\cos n \\theta_1 & \\sin n \\theta_1 \\\\\n+-\\sin n \\theta_1 & \\cos n \\theta_1\n+\\end{array}\\right) = \\left(\\begin{array}{cc}\n+\\cos (-n \\theta_1) & - \\sin (-n \\theta_1) \\\\\n+\\sin (-n \\theta_1) & \\cos (-n \\theta_1)\n+\\end{array}\\right)= {\\boldsymbol{R_{(-n\\theta_i)}}}\n+$$\n+\n+which means rotate $n\\theta_i$ clockwise. Therefore, the LHS of Eq.(35) can be written as:\n+\n+$$\n+\\left(\\boldsymbol{R}_{\\Theta, m}^d \\boldsymbol{W}_q \\boldsymbol{x}_m\\right)^{\\top}\\left(\\boldsymbol{R}_{\\Theta, n}^d \\boldsymbol{W}_k \\boldsymbol{x}_n\\right)=\\boldsymbol{q}^T\\boldsymbol{R}_{\\Theta, n-m}^d\\boldsymbol{k}  \\tag{4}\n+$$\n+\n+It's easy to see that the RHS of Eq.(4) is a **scalar**. We focus on $\\boldsymbol{q}^T\\boldsymbol{R}_{\\Theta, n-m}^d$ first. For index $2j, 2j+1, 0\\le j\\le d/2-1$ , It's easy to see that elements $[q_{2j},q_{2j+1}]$ in $\\boldsymbol{q}^T$ will only interact with the block $\\boldsymbol{R_{(n-m)\\theta_{j+1}}}$, and we have:\n+\n+$$\n+(\\boldsymbol{q}^T\\boldsymbol{R}_{\\Theta, n-m}^d)[2j:2j+1] = \\newline [q_{2j}cos(n-m)\\theta_{j+1}+q_{2j+1}sin(n-m)\\theta_{j+1}, -q_{2j}sin(n-m)\\theta_{j+1}+q_{2j+1}cos(n-m)\\theta_{j+1}]_{1\\times 2}\n+$$\n+\n+And these two elements will be multiplied with the $2j, 2j+1$ elements of $\\boldsymbol{k}$, then we finally get:\n+\n+$$\n+(q_{2j}k_{2j}+q_{2j+1}k_{2j+1})cos(n-m)\\theta_{j+1}+(q_{2j+1}k_{2j}-q_{2j}k_{2j+1})sin(n-m)\\theta_{j+1}\n+$$\n+\n+Then we back to the complex number form used by authors. In Eq.(35), it's said $\\boldsymbol{q}_{[2 i: 2 i+1]}\\cdot k_{[2 i: 2 i+1]}$ is **operated as complex multiplication**, which means:\n+\n+$$\n+\\begin{aligned}\n+\\boldsymbol{q}_{[2 i: 2 i+1]}\\cdot k_{[2 i: 2 i+1]}^* &= (q_{2i} + iq_{2i+1})\\cdot (k_{2i}+ik_{2i+1})^* \\newline\n+&= q_{2i}k_{2i}-q_{2i+1}k_{2i+1}+i(q_{2i}k_{2i+1}+q_{2i+1}k_{2i})\n+\\end{aligned}\n+$$\n+\n+## Other Interesting findings\n+\n ## References\n \n-[^1] Su, Jianlin, et al. \"Roformer: Enhanced transformer with rotary position embedding.\" Neurocomputing 568 (2024): 127063.\n+[^1]: Su, Jianlin, et al. \"Roformer: Enhanced transformer with rotary position embedding.\" Neurocomputing 568 (2024): 127063.\n+[^2]:\n+    苏剑林. (Mar. 23, 2021). 《Transformer 升级之路：2、博采众长的旋转式位置编码 》[Blog post]. Retrieved from https://spaces.ac.cn/archives/8265\n+    [^]: Ding, Yiran, et al. \"Longrope: Extending llm context window beyond 2 million tokens.\" arXiv preprint arXiv:2402.13753 (2024).\n \n-[2] Ding, Yiran, et al. \"Longrope: Extending llm context window beyond 2 million tokens.\" arXiv preprint arXiv:2402.13753 (2024).\n+## Desserts\n+\n+Jianlin Su is a very talented guy, his Blogs are all very insightful. Among his numerous blogs, a series called \"To upgrade Transformer\" **(Transformer 升级之路)** really deserve an In-depth reading. Here I curate a list, since the search engine in the blog website seems not to work well.\n+\n+[Transformer 升级之路：2、博采众长的旋转式位置编码](https://spaces.ac.cn/archives/8265)\n+\n+[Transformer 升级之路：7、长度外推性与局部注意力](https://spaces.ac.cn/archives/9431)\n+\n+[Transformer 升级之路：9、一种全局长度外推的新思路](https://spaces.ac.cn/archives/9603)\n+\n+[Transformer 升级之路：10、RoPE 是一种 β 进制编码](https://spaces.ac.cn/archives/9675)\n+\n+[Transformer 升级之路：18、RoPE 的底数选择原则](https://spaces.ac.cn/archives/10122)\n"
                },
                {
                    "date": 1720153820342,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -99,12 +99,14 @@\n \n $$\n \\begin{aligned}\n \\boldsymbol{q}_{[2 i: 2 i+1]}\\cdot k_{[2 i: 2 i+1]}^* &= (q_{2i} + iq_{2i+1})\\cdot (k_{2i}+ik_{2i+1})^* \\newline\n-&= q_{2i}k_{2i}-q_{2i+1}k_{2i+1}+i(q_{2i}k_{2i+1}+q_{2i+1}k_{2i})\n+&= (q_{2i}k_{2i}+q_{2i+1}k_{2i+1})+i(-q_{2i}k_{2i+1}+q_{2i+1}k_{2i}) \\tag{5}\n \\end{aligned}\n $$\n \n+Then if we multiply the RHS of Eq.(5) with $e^{i(m-n)\\theta_i}$, we will get the RHS of Eq.(35). Therefore, Eq.(35) is proved.\n+\n ## Other Interesting findings\n \n ## References\n \n"
                },
                {
                    "date": 1720153987687,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -85,9 +85,9 @@\n \n It's easy to see that the RHS of Eq.(4) is a **scalar**. We focus on $\\boldsymbol{q}^T\\boldsymbol{R}_{\\Theta, n-m}^d$ first. For index $2j, 2j+1, 0\\le j\\le d/2-1$ , It's easy to see that elements $[q_{2j},q_{2j+1}]$ in $\\boldsymbol{q}^T$ will only interact with the block $\\boldsymbol{R_{(n-m)\\theta_{j+1}}}$, and we have:\n \n $$\n-(\\boldsymbol{q}^T\\boldsymbol{R}_{\\Theta, n-m}^d)[2j:2j+1] = \\newline [q_{2j}cos(n-m)\\theta_{j+1}+q_{2j+1}sin(n-m)\\theta_{j+1}, -q_{2j}sin(n-m)\\theta_{j+1}+q_{2j+1}cos(n-m)\\theta_{j+1}]_{1\\times 2}\n+(\\boldsymbol{q}^T\\boldsymbol{R}_{\\Theta, n-m}^d)[2j:2j+1] = \\newline [q_{2j}cos(n-m)\\theta_{j+1}+q_{2j+1}sin(n-m)\\theta_{j+1}, -q_{2j}sin(n-m)\\theta_{j+1}+q_{2j+1}cos(n-m)\\theta_{j+1}]_{1\\times 2} \\tag{key}\n $$\n \n And these two elements will be multiplied with the $2j, 2j+1$ elements of $\\boldsymbol{k}$, then we finally get:\n \n@@ -103,10 +103,15 @@\n &= (q_{2i}k_{2i}+q_{2i+1}k_{2i+1})+i(-q_{2i}k_{2i+1}+q_{2i+1}k_{2i}) \\tag{5}\n \\end{aligned}\n $$\n \n-Then if we multiply the RHS of Eq.(5) with $e^{i(m-n)\\theta_i}$, we will get the RHS of Eq.(35). Therefore, Eq.(35) is proved.\n+Then if we multiply the RHS of Eq.(5) with $e^{i(m-n)\\theta_i}=cos(m-n)\\theta_i+i\\cdot sin(m-n)\\theta_i$, and focus on real part, we will magically get the Eq.(key). It's very elegant.\n \n+$$\n+\n+\n+$$\n+\n ## Other Interesting findings\n \n ## References\n \n"
                },
                {
                    "date": 1720154042740,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -57,8 +57,10 @@\n 0 & 0 & 0 & 0 & \\cdots & \\sin m \\theta_{d / 2} & \\cos m \\theta_{d / 2}\n \\end{array}\\right)_{d\\times d}\n $$\n \n+### Derivation of Eq.(35)\n+\n $\\theta_i$ belongs to $\\Theta=\\left\\{\\theta_i=10000^{-2(i-1) / d}, i \\in[1,2, \\ldots, d / 2]\\right\\}$​​, where 10000 is called **\"base\"**, as a predefined constant. The value of base will have interesting effect on RoPE, which we will discuss later.\n \n $\\boldsymbol{x}_i \\in \\mathbb{R}^d$ is the d-dimensional word embedding vector of token $w_i$ (Let $\\mathbb{S}_N=\\left\\{w_i\\right\\}_{i=1}^N$ be a sequence of $N$ input tokens with $w_i$ being the $i^{t h}$ element. The corresponding word embedding of $\\mathbb{S}_N$ is denoted as $\\mathbb{E}_N=\\left\\{\\boldsymbol{x}_i\\right\\}_{i=1}^N$). $\\boldsymbol{q}=\\boldsymbol{W}_q \\boldsymbol{x}_m$ and $\\boldsymbol{k}=\\boldsymbol{W}_k \\boldsymbol{x}_n$ are of the shape as $\\boldsymbol{x_i}$.\n \n@@ -103,15 +105,12 @@\n &= (q_{2i}k_{2i}+q_{2i+1}k_{2i+1})+i(-q_{2i}k_{2i+1}+q_{2i+1}k_{2i}) \\tag{5}\n \\end{aligned}\n $$\n \n-Then if we multiply the RHS of Eq.(5) with $e^{i(m-n)\\theta_i}=cos(m-n)\\theta_i+i\\cdot sin(m-n)\\theta_i$, and focus on real part, we will magically get the Eq.(key). It's very elegant.\n+Then if we multiply the RHS of Eq.(5) with $e^{i(m-n)\\theta_i}=cos(m-n)\\theta_i+i\\cdot sin(m-n)\\theta_i$, and focus on real part, we will magically get the Eq.(key), with $i$ substituted by $j$. It's quite elegant!\n \n-$$\n+We then sum up $j$\n \n-\n-$$\n-\n ## Other Interesting findings\n \n ## References\n \n"
                },
                {
                    "date": 1720154058503,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -107,9 +107,9 @@\n $$\n \n Then if we multiply the RHS of Eq.(5) with $e^{i(m-n)\\theta_i}=cos(m-n)\\theta_i+i\\cdot sin(m-n)\\theta_i$, and focus on real part, we will magically get the Eq.(key), with $i$ substituted by $j$. It's quite elegant!\n \n-We then sum up $j$\n+We then sum up $j$ from $0$ to $d/2-1$, then we successfully get Eq.(35).\n \n ## Other Interesting findings\n \n ## References\n"
                },
                {
                    "date": 1720154259697,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -57,14 +57,14 @@\n 0 & 0 & 0 & 0 & \\cdots & \\sin m \\theta_{d / 2} & \\cos m \\theta_{d / 2}\n \\end{array}\\right)_{d\\times d}\n $$\n \n-### Derivation of Eq.(35)\n-\n $\\theta_i$ belongs to $\\Theta=\\left\\{\\theta_i=10000^{-2(i-1) / d}, i \\in[1,2, \\ldots, d / 2]\\right\\}$​​, where 10000 is called **\"base\"**, as a predefined constant. The value of base will have interesting effect on RoPE, which we will discuss later.\n \n $\\boldsymbol{x}_i \\in \\mathbb{R}^d$ is the d-dimensional word embedding vector of token $w_i$ (Let $\\mathbb{S}_N=\\left\\{w_i\\right\\}_{i=1}^N$ be a sequence of $N$ input tokens with $w_i$ being the $i^{t h}$ element. The corresponding word embedding of $\\mathbb{S}_N$ is denoted as $\\mathbb{E}_N=\\left\\{\\boldsymbol{x}_i\\right\\}_{i=1}^N$). $\\boldsymbol{q}=\\boldsymbol{W}_q \\boldsymbol{x}_m$ and $\\boldsymbol{k}=\\boldsymbol{W}_k \\boldsymbol{x}_n$ are of the shape as $\\boldsymbol{x_i}$.\n \n+### Derivation of Eq.(35)\n+\n **Now let's first derive Eq.(35).** We first try to get **its complete form without using complex numbers**. Each block $\\boldsymbol{R_{m\\theta_i}}$ in matrix $\\boldsymbol{R_{\\theta, m}^d}$ is a rotation matrix, where $\\boldsymbol{R_{m\\theta_i}}=\\left(\\begin{array}{cc}\n \\cos m \\theta_1 & -\\sin m \\theta_1 \\\\\n \\sin m \\theta_1 & \\cos m \\theta_1\n \\end{array}\\right)$ means contrarotating $m\\theta_1$, while\n@@ -109,8 +109,12 @@\n Then if we multiply the RHS of Eq.(5) with $e^{i(m-n)\\theta_i}=cos(m-n)\\theta_i+i\\cdot sin(m-n)\\theta_i$, and focus on real part, we will magically get the Eq.(key), with $i$ substituted by $j$. It's quite elegant!\n \n We then sum up $j$ from $0$ to $d/2-1$, then we successfully get Eq.(35).\n \n+### Derivation of Eq.(36)\n+\n+The derivation of Eq.(36) is relatively simple. We just focus on $S_{i+1}$, and combine the same items will give us correct coefficient which is $h_{i+1}-h_i$. Then we sum up $i$ from $0$ to $d/2-1$, and we get Eq.(36).\n+\n ## Other Interesting findings\n \n ## References\n \n"
                },
                {
                    "date": 1720154582450,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -105,16 +105,20 @@\n &= (q_{2i}k_{2i}+q_{2i+1}k_{2i+1})+i(-q_{2i}k_{2i+1}+q_{2i+1}k_{2i}) \\tag{5}\n \\end{aligned}\n $$\n \n-Then if we multiply the RHS of Eq.(5) with $e^{i(m-n)\\theta_i}=cos(m-n)\\theta_i+i\\cdot sin(m-n)\\theta_i$, and focus on real part, we will magically get the Eq.(key), with $i$ substituted by $j$. It's quite elegant!\n+(Please don't confuse your self with the index $i$ and $i$ representing imaginary number). Then if we multiply the RHS of Eq.(5) with $e^{i(m-n)\\theta_i}=cos(m-n)\\theta_i+i\\cdot sin(m-n)\\theta_i$, and focus on real part, we will magically get the Eq.(key), with $i$ substituted by $j$. It's quite elegant!\n \n We then sum up $j$ from $0$ to $d/2-1$, then we successfully get Eq.(35).\n \n ### Derivation of Eq.(36)\n \n The derivation of Eq.(36) is relatively simple. We just focus on $S_{i+1}$, and combine the same items will give us correct coefficient which is $h_{i+1}-h_i$. Then we sum up $i$ from $0$ to $d/2-1$, and we get Eq.(36).\n \n+### Why Long-term Decay?\n+\n+Well, then we arrive at the most important part, we need to prove the value of $\\frac{1}{d / 2} \\sum_{j=1}^{d / 2}\\left|S_j\\right|$ decay with the relative distance $m-n$ increases by setting $\\theta_i=10000^{-2 i / d}$, where $S_j=$ $\\sum_{i=0}^{j-1} e^{i(m-n) \\theta_i}$\n+\n ## Other Interesting findings\n \n ## References\n \n"
                },
                {
                    "date": 1720154609226,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -105,9 +105,9 @@\n &= (q_{2i}k_{2i}+q_{2i+1}k_{2i+1})+i(-q_{2i}k_{2i+1}+q_{2i+1}k_{2i}) \\tag{5}\n \\end{aligned}\n $$\n \n-(Please don't confuse your self with the index $i$ and $i$ representing imaginary number). Then if we multiply the RHS of Eq.(5) with $e^{i(m-n)\\theta_i}=cos(m-n)\\theta_i+i\\cdot sin(m-n)\\theta_i$, and focus on real part, we will magically get the Eq.(key), with $i$ substituted by $j$. It's quite elegant!\n+**_(Please don't confuse your self with the index $i$ and $i$ representing imaginary number)_**. Then if we multiply the RHS of Eq.(5) with $e^{i(m-n)\\theta_i}=cos(m-n)\\theta_i+i\\cdot sin(m-n)\\theta_i$, and focus on real part, we will magically get the Eq.(key), with $i$ substituted by $j$. It's quite elegant!\n \n We then sum up $j$ from $0$ to $d/2-1$, then we successfully get Eq.(35).\n \n ### Derivation of Eq.(36)\n"
                },
                {
                    "date": 1720154680868,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -87,9 +87,9 @@\n \n It's easy to see that the RHS of Eq.(4) is a **scalar**. We focus on $\\boldsymbol{q}^T\\boldsymbol{R}_{\\Theta, n-m}^d$ first. For index $2j, 2j+1, 0\\le j\\le d/2-1$ , It's easy to see that elements $[q_{2j},q_{2j+1}]$ in $\\boldsymbol{q}^T$ will only interact with the block $\\boldsymbol{R_{(n-m)\\theta_{j+1}}}$, and we have:\n \n $$\n-(\\boldsymbol{q}^T\\boldsymbol{R}_{\\Theta, n-m}^d)[2j:2j+1] = \\newline [q_{2j}cos(n-m)\\theta_{j+1}+q_{2j+1}sin(n-m)\\theta_{j+1}, -q_{2j}sin(n-m)\\theta_{j+1}+q_{2j+1}cos(n-m)\\theta_{j+1}]_{1\\times 2} \\tag{key}\n+(\\boldsymbol{q}^T\\boldsymbol{R}_{\\Theta, n-m}^d)[2j:2j+1] = [q_{2j}cos(n-m)\\theta_{j+1}+q_{2j+1}sin(n-m)\\theta_{j+1}, -q_{2j}sin(n-m)\\theta_{j+1}+q_{2j+1}cos(n-m)\\theta_{j+1}]_{1\\times 2} \\tag{key}\n $$\n \n And these two elements will be multiplied with the $2j, 2j+1$ elements of $\\boldsymbol{k}$, then we finally get:\n \n@@ -101,10 +101,10 @@\n \n $$\n \\begin{aligned}\n \\boldsymbol{q}_{[2 i: 2 i+1]}\\cdot k_{[2 i: 2 i+1]}^* &= (q_{2i} + iq_{2i+1})\\cdot (k_{2i}+ik_{2i+1})^* \\newline\n-&= (q_{2i}k_{2i}+q_{2i+1}k_{2i+1})+i(-q_{2i}k_{2i+1}+q_{2i+1}k_{2i}) \n-\\end{aligned} \\tag{5}\n+&= (q_{2i}k_{2i}+q_{2i+1}k_{2i+1})+i(-q_{2i}k_{2i+1}+q_{2i+1}k_{2i}) \\tag{5}\n+\\end{aligned}\n $$\n \n **_(Please don't confuse your self with the index $i$ and $i$ representing imaginary number)_**. Then if we multiply the RHS of Eq.(5) with $e^{i(m-n)\\theta_i}=cos(m-n)\\theta_i+i\\cdot sin(m-n)\\theta_i$, and focus on real part, we will magically get the Eq.(key), with $i$ substituted by $j$. It's quite elegant!\n \n"
                }
            ],
            "date": 1720056780184,
            "name": "Commit-0",
            "content": "---\ntitle: \"The Proof of Long-Term Decay Property of ROPE\"\ntags: [\"paper\"]\ndate: 2024\npath: \"posts/paper\"\nexcerpt: paperlist\n---\n\n## LLM\n\n### Token reward\n"
        }
    ]
}