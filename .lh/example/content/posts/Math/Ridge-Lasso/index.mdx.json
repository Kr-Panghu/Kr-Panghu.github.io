{
    "sourceFile": "example/content/posts/Math/Ridge-Lasso/index.mdx",
    "activeCommit": 0,
    "commits": [
        {
            "activePatchIndex": 16,
            "patches": [
                {
                    "date": 1718096715249,
                    "content": "Index: \n===================================================================\n--- \n+++ \n"
                },
                {
                    "date": 1718096733026,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -3,9 +3,8 @@\n tags: [\"ML\"]\n date: 2024-06-11\n cover: \"./preview.png\"\n path: \"posts/Math/Ridge-Lasso\"\n-excerpt: I started to build this personal website yesterday.\n ---\n \n I started to build this personal website yesterday.\n \n"
                },
                {
                    "date": 1718096969511,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -5,103 +5,21 @@\n cover: \"./preview.png\"\n path: \"posts/Math/Ridge-Lasso\"\n ---\n \n-I started to build this personal website yesterday.\n+In Machine learning, Ridge and Lasso are two common regularization methods.\n \n-## $\\LaTeX$ Support\n+The equation of Ridge and Lasso are as follows:\n \n-LaTeX test:\n+Ridge: $L_{ridge} = \\sum_{i=1}^{n} (y_i - \\beta_0 - \\sum_{j=1}^{p} \\beta_j x_{ij})^2 + \\lambda \\sum_{j=1}^{p} \\beta_j^2$\n \n-$$x^3+y^3=z^3$$\n+Lasso: $L_{lasso} = \\sum_{i=1}^{n} (y_i - \\beta_0 - \\sum_{j=1}^{p} \\beta_j x_{ij})^2 + \\lambda \\sum_{j=1}^{p} |\\beta_j|$\n \n-Though I'm quite satisfied showing my work on [GitHub](https://github.com/sleepymalc), it turns out that I have much more want to show. For example, GitHub is extremely unfriendly to show `.pdf` files since the internal link (we call it _hyperlink_ for those $\\LaTeX$ geeks) doesn't work. This kind\n-of problem arises quite often. Besides, my only hobby these days is _photographing_, and as you may already know, no one will show their photos on GitHub!\n+We know that Ridge will penalize those dimensions with large $\\beta$, which results in a dense solution. Lasso will penalize those dimensions with small $\\beta$, which results in a sparse solution.\n \n-Apart from all these, since I use $\\LaTeX$ a **LOT** (yup, I'm one of those $\\LaTeX$ Freaks), and the only way I know how to render $\\LaTeX$ formula on GitHub is _ugly_, and it can only render fixed color texts, hence this makes me made my mind eventually.\n-For example, we can easily print out some beautiful formula like\n+However, what if we combine Ridge and Lasso together? That is, we use the following equation:\n \n-<!-- $$\\begin{tikzcd}\n-\t\t\tS & {G_\\alpha} \\\\\n-\t\t\t{G_\\beta} & {G_\\alpha \\ast_S G_\\beta} \\\\\n-\t\t\t&& X\n-\t\t\t\\arrow[\"{i_{\\alpha \\beta} }\", from=1-1, to=1-2]\n-\t\t\t\\arrow[\"{i_{\\beta \\alpha} }\"', from=1-1, to=2-1]\n-\t\t\t\\arrow[from=1-2, to=2-2]\n-\t\t\t\\arrow[from=2-1, to=2-2]\n-\t\t\t\\arrow[\"{\\exists !}\", dashed, from=2-2, to=3-3]\n-\t\t\t\\arrow[curve={height=-12pt}, from=1-2, to=3-3]\n-\t\t\t\\arrow[curve={height=12pt}, from=2-1, to=3-3]\n-\t\t\\end{tikzcd}$$ -->\n-\n-which is just a simple [commutative diagram](https://en.wikipedia.org/wiki/Commutative_diagram) for [Seifert–Van Kampen theorem](https://en.wikipedia.org/wiki/Seifert%E2%80%93Van_Kampen_theorem), or like\n-\n $$\n-\\lim\\limits_{r \\to 0} \\frac{1}{m(B(x, r))}\\int_{B(x, r)}\\left\\vert f(y) - f(x) \\right\\vert\\,\\mathrm{d}y = 0 \\text{for a.e. \\(x\\)}\n+\\mathcal{L}=\\|Y-X \\beta\\|_2^2+\\lambda_1\\|\\beta\\|^2+\\lambda_2\\|\\beta\\|_1\n $$\n \n-for $f\\in L^1$, which is the statement of [Lebesgue Differentiation Theorem](https://en.wikipedia.org/wiki/Lebesgue_differentiation_theorem).\n-\n-## Gist Support\n-\n-```json {numberLines: true}\n-{\n-  \"openapi\": \"3.0.2\",\n-  \"info\": {\n-    \"title\": \"test\",\n-    \"description\": \"test\",\n-    \"version\": \"0.1.0\"\n-  },\n-  \"paths\": {\n-    \"/\": {\n-      \"get\": {\n-        \"summary\": \"test\",\n-        \"operationId\": \"test\",\n-        \"parameters\": [\n-          {\n-            \"required\": false,\n-            \"schema\": {\n-              \"allOf\": [\n-                {\n-                  \"$ref\": \"#/components/schemas/ArchiveType\"\n-                }\n-              ],\n-              \"default\": \"zip\"\n-            },\n-            \"name\": \"archiveType\",\n-            \"in\": \"query\"\n-          }\n-        ],\n-        \"responses\": {\n-          \"200\": {\n-            \"description\": \"Successful Response\",\n-            \"content\": {\n-              \"application/json\": {\n-                \"schema\": {}\n-              }\n-            }\n-          }\n-        }\n-      }\n-    }\n-  },\n-  \"components\": {\n-    \"schemas\": {\n-      \"ArchiveType\": {\n-        \"title\": \"ArchiveType\",\n-        \"enum\": [\"zip\", \"tar\", \"unknown\"],\n-        \"type\": \"string\",\n-        \"description\": \"An enumeration.\"\n-      }\n-    }\n-  }\n-}\n-```\n-\n-[//]: # '<script src=\"https://gist.github.com/tc-imba/76dc7e627a56ac84b5c1c14b08d90f1e.js\"></script>'\n-[//]: # \"`gist:tc-imba/76dc7e627a56ac84b5c1c14b08d90f1e`\"\n-[//]: #\n-[//]: # \"`gist:weirdpattern/ce54fdb1e5621b5966e146026995b974#syntax.text`\"\n-\n-<Gist id=\"bedde975e6e92a77e2321487ba45f313\" />\n-\n-[//]: # \"<Gist url='https://gist.github.com/GeorgeGkas/5f55a83909a3f5b766934ffe802d30df#file-start-js' />\"\n+Then what will happen to the $\\beta$? Will it be sparse or dense?\n"
                },
                {
                    "date": 1718097486430,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1,6 +1,6 @@\n ---\n-title: \"The influence on $\\beta$ when combining Rigde and Lasso Regularization\"\n+title: \"Combining Rigde and Lasso Regularization\"\n tags: [\"ML\"]\n date: 2024-06-11\n cover: \"./preview.png\"\n path: \"posts/Math/Ridge-Lasso\"\n"
                },
                {
                    "date": 1718097633618,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -3,8 +3,9 @@\n tags: [\"ML\"]\n date: 2024-06-11\n cover: \"./preview.png\"\n path: \"posts/Math/Ridge-Lasso\"\n+excerpt: \"What will happen to $\\beta$ if we combine Ridge and Lasso regularization together?\"\n ---\n \n In Machine learning, Ridge and Lasso are two common regularization methods.\n \n"
                },
                {
                    "date": 1718097640294,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1,6 +1,6 @@\n ---\n-title: \"Combining Rigde and Lasso Regularization\"\n+title: \"Elastic Net: Combining Rigde and Lasso Regularization\"\n tags: [\"ML\"]\n date: 2024-06-11\n cover: \"./preview.png\"\n path: \"posts/Math/Ridge-Lasso\"\n"
                },
                {
                    "date": 1718097781045,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1,6 +1,6 @@\n ---\n-title: \"Elastic Net: Combining Rigde and Lasso Regularization\"\n+title: \"Elastic Net Regularization: Combining Rigde and Lasso\"\n tags: [\"ML\"]\n date: 2024-06-11\n cover: \"./preview.png\"\n path: \"posts/Math/Ridge-Lasso\"\n"
                },
                {
                    "date": 1718097797831,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -19,8 +19,8 @@\n \n However, what if we combine Ridge and Lasso together? That is, we use the following equation:\n \n $$\n-\\mathcal{L}=\\|Y-X \\beta\\|_2^2+\\lambda_1\\|\\beta\\|^2+\\lambda_2\\|\\beta\\|_1\n+\\mathcal{L}=\\|Y-X \\beta\\|_2^2+\\lambda_2\\|\\beta\\|^2+\\lambda_1\\|\\beta\\|_1\n $$\n \n Then what will happen to the $\\beta$? Will it be sparse or dense?\n"
                },
                {
                    "date": 1718099017874,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -6,8 +6,10 @@\n path: \"posts/Math/Ridge-Lasso\"\n excerpt: \"What will happen to $\\beta$ if we combine Ridge and Lasso regularization together?\"\n ---\n \n+## Introduction\n+\n In Machine learning, Ridge and Lasso are two common regularization methods.\n \n The equation of Ridge and Lasso are as follows:\n \n@@ -23,4 +25,12 @@\n \\mathcal{L}=\\|Y-X \\beta\\|_2^2+\\lambda_2\\|\\beta\\|^2+\\lambda_1\\|\\beta\\|_1\n $$\n \n Then what will happen to the $\\beta$? Will it be sparse or dense?\n+\n+## Elastic Net Regularization\n+\n+Actually, the combined regularization is called Elastic Net Regularization. It is a linear combination of L1 and L2 regularization. It combines the advantages of Ridge and Lasso regularization, which can\n+\n+## Reference\n+\n+> https://en.wikipedia.org/wiki/Elastic_net_regularization > https://blog.csdn.net/qq_51320133/article/details/137421397\n"
                },
                {
                    "date": 1718099050272,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -28,9 +28,19 @@\n Then what will happen to the $\\beta$? Will it be sparse or dense?\n \n ## Elastic Net Regularization\n \n-Actually, the combined regularization is called Elastic Net Regularization. It is a linear combination of L1 and L2 regularization. It combines the advantages of Ridge and Lasso regularization, which can\n+Actually, the combined regularization is called Elastic Net Regularization. It is a linear combination of L1 and L2 regularization. It combines the advantages of Ridge and Lasso regularization.\n \n+**Advantages:**\n+\n+- Elastic Net combines the strengths of both Lasso and Ridge. It can select features among a large number of redundant features and also handle highly correlated features.\n+- When there is multicollinearity among features, Elastic Net can select a group of correlated features to participate together in model building, enhancing model stability.\n+\n+**Disadvantages:**\n+\n+- Parameter tuning, such as the regularization strength α and the L1/L2 weights ρ, needs to be carefully selected through methods like cross-validation; otherwise, it might lead to poor model performance.\n+- Compared to Lasso alone, Elastic Net's solutions may not be sparse enough in extremely sparse problems, which could reduce model interpretability.\n+\n ## Reference\n \n > https://en.wikipedia.org/wiki/Elastic_net_regularization > https://blog.csdn.net/qq_51320133/article/details/137421397\n"
                },
                {
                    "date": 1718099056519,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -42,5 +42,7 @@\n - Compared to Lasso alone, Elastic Net's solutions may not be sparse enough in extremely sparse problems, which could reduce model interpretability.\n \n ## Reference\n \n-> https://en.wikipedia.org/wiki/Elastic_net_regularization > https://blog.csdn.net/qq_51320133/article/details/137421397\n+> https://en.wikipedia.org/wiki/Elastic_net_regularization\n+\n+> https://blog.csdn.net/qq_51320133/article/details/137421397\n"
                },
                {
                    "date": 1718107639010,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -43,6 +43,5 @@\n \n ## Reference\n \n > https://en.wikipedia.org/wiki/Elastic_net_regularization\n-\n > https://blog.csdn.net/qq_51320133/article/details/137421397\n"
                },
                {
                    "date": 1718113771908,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1,11 +1,11 @@\n ---\n-title: \"Elastic Net Regularization: Combining Rigde and Lasso\"\n+title: \"Machine Learning Review\"\n tags: [\"ML\"]\n date: 2024-06-11\n cover: \"./preview.png\"\n path: \"posts/Math/Ridge-Lasso\"\n-excerpt: \"What will happen to $\\beta$ if we combine Ridge and Lasso regularization together?\"\n+excerpt: \"Only 5 days to the final exam, let's review the knowledge of Machine Learning.\"\n ---\n \n ## Introduction\n \n@@ -42,6 +42,5 @@\n - Compared to Lasso alone, Elastic Net's solutions may not be sparse enough in extremely sparse problems, which could reduce model interpretability.\n \n ## Reference\n \n-> https://en.wikipedia.org/wiki/Elastic_net_regularization\n-> https://blog.csdn.net/qq_51320133/article/details/137421397\n+> https://en.wikipedia.org/wiki/Elastic_net_regularization > https://blog.csdn.net/qq_51320133/article/details/137421397\n"
                },
                {
                    "date": 1718113781228,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -3,9 +3,9 @@\n tags: [\"ML\"]\n date: 2024-06-11\n cover: \"./preview.png\"\n path: \"posts/Math/Ridge-Lasso\"\n-excerpt: \"Only 5 days to the final exam, let's review the knowledge of Machine Learning.\"\n+excerpt: \"Only 3 days left to the final exam, let's review the knowledge of Machine Learning.\"\n ---\n \n ## Introduction\n \n"
                },
                {
                    "date": 1718114032264,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -6,9 +6,9 @@\n path: \"posts/Math/Ridge-Lasso\"\n excerpt: \"Only 3 days left to the final exam, let's review the knowledge of Machine Learning.\"\n ---\n \n-## Introduction\n+## Elastic Net Regularization\n \n In Machine learning, Ridge and Lasso are two common regularization methods.\n \n The equation of Ridge and Lasso are as follows:\n@@ -26,10 +26,8 @@\n $$\n \n Then what will happen to the $\\beta$? Will it be sparse or dense?\n \n-## Elastic Net Regularization\n-\n Actually, the combined regularization is called Elastic Net Regularization. It is a linear combination of L1 and L2 regularization. It combines the advantages of Ridge and Lasso regularization.\n \n **Advantages:**\n \n@@ -40,7 +38,23 @@\n \n - Parameter tuning, such as the regularization strength α and the L1/L2 weights ρ, needs to be carefully selected through methods like cross-validation; otherwise, it might lead to poor model performance.\n - Compared to Lasso alone, Elastic Net's solutions may not be sparse enough in extremely sparse problems, which could reduce model interpretability.\n \n+## KL Divergence\n+\n+When reviewing the Variational Autoencoder, there is a equation in the derivation process:\n+\n+$$\n+\\mathrm{KL}\\left(P_{\\text {data }}(X) \\mid p_\\theta(X)\\right)+\\operatorname{KL}\\left(q_\\phi(h \\mid X) \\mid p_\\theta(h \\mid X)\\right)=\\operatorname{KL}\\left(P_{\\text {data }}(X) q_\\phi(h \\mid X) \\mid p_\\theta(h, X)\\right)\n+$$\n+\n+We know that, for KL Divergence, we have:\n+\n+- $\\mathrm{KL}(p(y \\mid x) \\mid q(y \\mid x)) \\stackrel{\\text { def }}{=} \\mathrm{E}_{p(x)} \\mathrm{E}_{p(y \\mid x)}\\left[\\log \\frac{p(Y \\mid X)}{q(Y \\mid X)}\\right]$.\n+- $\\mathrm{KL}(p(x, y) \\mid q(x, y))=\\operatorname{KL}(p(x) \\mid q(x))+\\mathrm{KL}(p(y \\mid x) \\mid q(y \\mid x))$\n+\n+But why the KL Divergence in the derivation of VAE is like this? What is the intuitive explanation of this equation?\n+\n ## Reference\n \n-> https://en.wikipedia.org/wiki/Elastic_net_regularization > https://blog.csdn.net/qq_51320133/article/details/137421397\n+> https://en.wikipedia.org/wiki/Elastic_net_regularization\n+> https://blog.csdn.net/qq_51320133/article/details/137421397\n"
                },
                {
                    "date": 1718114270253,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -0,0 +1,61 @@\n+---\n+title: \"Machine Learning Review\"\n+tags: [\"ML\"]\n+date: 2024-06-11\n+cover: \"./preview.png\"\n+path: \"posts/Math/Ridge-Lasso\"\n+excerpt: \"Only 3 days left to the final exam, let's review the knowledge of Machine Learning.\"\n+---\n+\n+## Elastic Net Regularization\n+\n+In Machine learning, Ridge and Lasso are two common regularization methods.\n+\n+The equation of Ridge and Lasso are as follows:\n+\n+Ridge: $L_{ridge} = \\sum_{i=1}^{n} (y_i - \\beta_0 - \\sum_{j=1}^{p} \\beta_j x_{ij})^2 + \\lambda \\sum_{j=1}^{p} \\beta_j^2$\n+\n+Lasso: $L_{lasso} = \\sum_{i=1}^{n} (y_i - \\beta_0 - \\sum_{j=1}^{p} \\beta_j x_{ij})^2 + \\lambda \\sum_{j=1}^{p} |\\beta_j|$\n+\n+We know that Ridge will penalize those dimensions with large $\\beta$, which results in a dense solution. Lasso will penalize those dimensions with small $\\beta$, which results in a sparse solution.\n+\n+However, what if we combine Ridge and Lasso together? That is, we use the following equation:\n+\n+$$\n+\\mathcal{L}=\\|Y-X \\beta\\|_2^2+\\lambda_2\\|\\beta\\|^2+\\lambda_1\\|\\beta\\|_1\n+$$\n+\n+Then what will happen to the $\\beta$? Will it be sparse or dense?\n+\n+Actually, the combined regularization is called Elastic Net Regularization[^1]. It is a linear combination of L1 and L2 regularization. It combines the advantages of Ridge and Lasso regularization.\n+\n+**Advantages:**\n+\n+- Elastic Net combines the strengths of both Lasso and Ridge. It can select features among a large number of redundant features and also handle highly correlated features.\n+- When there is multicollinearity among features, Elastic Net can select a group of correlated features to participate together in model building, enhancing model stability.\n+\n+**Disadvantages:**\n+\n+- Parameter tuning, such as the regularization strength α and the L1/L2 weights ρ, needs to be carefully selected through methods like cross-validation; otherwise, it might lead to poor model performance.\n+- Compared to Lasso alone, Elastic Net's solutions may not be sparse enough in extremely sparse problems, which could reduce model interpretability.\n+\n+## KL Divergence\n+\n+When reviewing the Variational Autoencoder, there is a equation in the derivation process:\n+\n+$$\n+\\mathrm{KL}\\left(P_{\\text {data }}(X) \\mid p_\\theta(X)\\right)+\\operatorname{KL}\\left(q_\\phi(h \\mid X) \\mid p_\\theta(h \\mid X)\\right)=\\operatorname{KL}\\left(P_{\\text {data }}(X) q_\\phi(h \\mid X) \\mid p_\\theta(h, X)\\right)\n+$$\n+\n+We know that, for KL Divergence, we have:\n+\n+- $\\mathrm{KL}(p(y \\mid x) \\mid q(y \\mid x)) \\stackrel{\\text { def }}{=} \\mathrm{E}_{p(x)} \\mathrm{E}_{p(y \\mid x)}\\left[\\log \\frac{p(Y \\mid X)}{q(Y \\mid X)}\\right]$.\n+- $\\mathrm{KL}(p(x, y) \\mid q(x, y))=\\operatorname{KL}(p(x) \\mid q(x))+\\mathrm{KL}(p(y \\mid x) \\mid q(y \\mid x))$\n+\n+But why the KL Divergence in the derivation of VAE is like this? What is the intuitive explanation of this equation?\n+\n+## Reference\n+\n+[^1] https://en.wikipedia.org/wiki/Elastic_net_regularization\n+\n+> https://blog.csdn.net/qq_51320133/article/details/137421397\n"
                },
                {
                    "date": 1718114304926,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -26,9 +26,9 @@\n $$\n \n Then what will happen to the $\\beta$? Will it be sparse or dense?\n \n-Actually, the combined regularization is called Elastic Net Regularization[^1]. It is a linear combination of L1 and L2 regularization. It combines the advantages of Ridge and Lasso regularization.\n+Actually, the combined regularization is called Elastic Net Regularization[^1]. It is a linear combination of L1 and L2 regularization, which has pros and cons as follow[^2]:\n \n **Advantages:**\n \n - Elastic Net combines the strengths of both Lasso and Ridge. It can select features among a large number of redundant features and also handle highly correlated features.\n@@ -56,66 +56,5 @@\n \n ## Reference\n \n [^1] https://en.wikipedia.org/wiki/Elastic_net_regularization\n-\n-> https://blog.csdn.net/qq_51320133/article/details/137421397\n----\n-title: \"Machine Learning Review\"\n-tags: [\"ML\"]\n-date: 2024-06-11\n-cover: \"./preview.png\"\n-path: \"posts/Math/Ridge-Lasso\"\n-excerpt: \"Only 3 days left to the final exam, let's review the knowledge of Machine Learning.\"\n----\n-\n-## Elastic Net Regularization\n-\n-In Machine learning, Ridge and Lasso are two common regularization methods.\n-\n-The equation of Ridge and Lasso are as follows:\n-\n-Ridge: $L_{ridge} = \\sum_{i=1}^{n} (y_i - \\beta_0 - \\sum_{j=1}^{p} \\beta_j x_{ij})^2 + \\lambda \\sum_{j=1}^{p} \\beta_j^2$\n-\n-Lasso: $L_{lasso} = \\sum_{i=1}^{n} (y_i - \\beta_0 - \\sum_{j=1}^{p} \\beta_j x_{ij})^2 + \\lambda \\sum_{j=1}^{p} |\\beta_j|$\n-\n-We know that Ridge will penalize those dimensions with large $\\beta$, which results in a dense solution. Lasso will penalize those dimensions with small $\\beta$, which results in a sparse solution.\n-\n-However, what if we combine Ridge and Lasso together? That is, we use the following equation:\n-\n-$$\n-\\mathcal{L}=\\|Y-X \\beta\\|_2^2+\\lambda_2\\|\\beta\\|^2+\\lambda_1\\|\\beta\\|_1\n-$$\n-\n-Then what will happen to the $\\beta$? Will it be sparse or dense?\n-\n-Actually, the combined regularization is called Elastic Net Regularization. It is a linear combination of L1 and L2 regularization. It combines the advantages of Ridge and Lasso regularization.\n-\n-**Advantages:**\n-\n-- Elastic Net combines the strengths of both Lasso and Ridge. It can select features among a large number of redundant features and also handle highly correlated features.\n-- When there is multicollinearity among features, Elastic Net can select a group of correlated features to participate together in model building, enhancing model stability.\n-\n-**Disadvantages:**\n-\n-- Parameter tuning, such as the regularization strength α and the L1/L2 weights ρ, needs to be carefully selected through methods like cross-validation; otherwise, it might lead to poor model performance.\n-- Compared to Lasso alone, Elastic Net's solutions may not be sparse enough in extremely sparse problems, which could reduce model interpretability.\n-\n-## KL Divergence\n-\n-When reviewing the Variational Autoencoder, there is a equation in the derivation process:\n-\n-$$\n-\\mathrm{KL}\\left(P_{\\text {data }}(X) \\mid p_\\theta(X)\\right)+\\operatorname{KL}\\left(q_\\phi(h \\mid X) \\mid p_\\theta(h \\mid X)\\right)=\\operatorname{KL}\\left(P_{\\text {data }}(X) q_\\phi(h \\mid X) \\mid p_\\theta(h, X)\\right)\n-$$\n-\n-We know that, for KL Divergence, we have:\n-\n-- $\\mathrm{KL}(p(y \\mid x) \\mid q(y \\mid x)) \\stackrel{\\text { def }}{=} \\mathrm{E}_{p(x)} \\mathrm{E}_{p(y \\mid x)}\\left[\\log \\frac{p(Y \\mid X)}{q(Y \\mid X)}\\right]$.\n-- $\\mathrm{KL}(p(x, y) \\mid q(x, y))=\\operatorname{KL}(p(x) \\mid q(x))+\\mathrm{KL}(p(y \\mid x) \\mid q(y \\mid x))$\n-\n-But why the KL Divergence in the derivation of VAE is like this? What is the intuitive explanation of this equation?\n-\n-## Reference\n-\n-> https://en.wikipedia.org/wiki/Elastic_net_regularization\n-> https://blog.csdn.net/qq_51320133/article/details/137421397\n+[^2] https://blog.csdn.net/qq_51320133/article/details/137421397\n"
                }
            ],
            "date": 1718096715249,
            "name": "Commit-0",
            "content": "---\ntitle: \"The influence on $\\beta$ when combining Rigde and Lasso Regularization\"\ntags: [\"ML\"]\ndate: 2024-06-11\ncover: \"./preview.png\"\npath: \"posts/Math/Ridge-Lasso\"\nexcerpt: I started to build this personal website yesterday.\n---\n\nI started to build this personal website yesterday.\n\n## $\\LaTeX$ Support\n\nLaTeX test:\n\n$$x^3+y^3=z^3$$\n\nThough I'm quite satisfied showing my work on [GitHub](https://github.com/sleepymalc), it turns out that I have much more want to show. For example, GitHub is extremely unfriendly to show `.pdf` files since the internal link (we call it _hyperlink_ for those $\\LaTeX$ geeks) doesn't work. This kind\nof problem arises quite often. Besides, my only hobby these days is _photographing_, and as you may already know, no one will show their photos on GitHub!\n\nApart from all these, since I use $\\LaTeX$ a **LOT** (yup, I'm one of those $\\LaTeX$ Freaks), and the only way I know how to render $\\LaTeX$ formula on GitHub is _ugly_, and it can only render fixed color texts, hence this makes me made my mind eventually.\nFor example, we can easily print out some beautiful formula like\n\n<!-- $$\\begin{tikzcd}\n\t\t\tS & {G_\\alpha} \\\\\n\t\t\t{G_\\beta} & {G_\\alpha \\ast_S G_\\beta} \\\\\n\t\t\t&& X\n\t\t\t\\arrow[\"{i_{\\alpha \\beta} }\", from=1-1, to=1-2]\n\t\t\t\\arrow[\"{i_{\\beta \\alpha} }\"', from=1-1, to=2-1]\n\t\t\t\\arrow[from=1-2, to=2-2]\n\t\t\t\\arrow[from=2-1, to=2-2]\n\t\t\t\\arrow[\"{\\exists !}\", dashed, from=2-2, to=3-3]\n\t\t\t\\arrow[curve={height=-12pt}, from=1-2, to=3-3]\n\t\t\t\\arrow[curve={height=12pt}, from=2-1, to=3-3]\n\t\t\\end{tikzcd}$$ -->\n\nwhich is just a simple [commutative diagram](https://en.wikipedia.org/wiki/Commutative_diagram) for [Seifert–Van Kampen theorem](https://en.wikipedia.org/wiki/Seifert%E2%80%93Van_Kampen_theorem), or like\n\n$$\n\\lim\\limits_{r \\to 0} \\frac{1}{m(B(x, r))}\\int_{B(x, r)}\\left\\vert f(y) - f(x) \\right\\vert\\,\\mathrm{d}y = 0 \\text{for a.e. \\(x\\)}\n$$\n\nfor $f\\in L^1$, which is the statement of [Lebesgue Differentiation Theorem](https://en.wikipedia.org/wiki/Lebesgue_differentiation_theorem).\n\n## Gist Support\n\n```json {numberLines: true}\n{\n  \"openapi\": \"3.0.2\",\n  \"info\": {\n    \"title\": \"test\",\n    \"description\": \"test\",\n    \"version\": \"0.1.0\"\n  },\n  \"paths\": {\n    \"/\": {\n      \"get\": {\n        \"summary\": \"test\",\n        \"operationId\": \"test\",\n        \"parameters\": [\n          {\n            \"required\": false,\n            \"schema\": {\n              \"allOf\": [\n                {\n                  \"$ref\": \"#/components/schemas/ArchiveType\"\n                }\n              ],\n              \"default\": \"zip\"\n            },\n            \"name\": \"archiveType\",\n            \"in\": \"query\"\n          }\n        ],\n        \"responses\": {\n          \"200\": {\n            \"description\": \"Successful Response\",\n            \"content\": {\n              \"application/json\": {\n                \"schema\": {}\n              }\n            }\n          }\n        }\n      }\n    }\n  },\n  \"components\": {\n    \"schemas\": {\n      \"ArchiveType\": {\n        \"title\": \"ArchiveType\",\n        \"enum\": [\"zip\", \"tar\", \"unknown\"],\n        \"type\": \"string\",\n        \"description\": \"An enumeration.\"\n      }\n    }\n  }\n}\n```\n\n[//]: # '<script src=\"https://gist.github.com/tc-imba/76dc7e627a56ac84b5c1c14b08d90f1e.js\"></script>'\n[//]: # \"`gist:tc-imba/76dc7e627a56ac84b5c1c14b08d90f1e`\"\n[//]: #\n[//]: # \"`gist:weirdpattern/ce54fdb1e5621b5966e146026995b974#syntax.text`\"\n\n<Gist id=\"bedde975e6e92a77e2321487ba45f313\" />\n\n[//]: # \"<Gist url='https://gist.github.com/GeorgeGkas/5f55a83909a3f5b766934ffe802d30df#file-start-js' />\"\n"
        }
    ]
}